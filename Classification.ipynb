{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data management\n",
    "import pandas as pd\n",
    "\n",
    "# Data preprocessing and trasformation (ETL)\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler, FunctionTransformer, Binarizer, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Math and Stat modules\n",
    "import numpy as np\n",
    "from scipy.stats import sem\n",
    "\n",
    "# Supervised Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, KFold, StratifiedKFold, RepeatedKFold, ShuffleSplit, StratifiedShuffleSplit\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve, roc_curve\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Unsupervised Learning\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepariamo velocemente i dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_data = pd.read_csv('BankChurnersMissingData.csv')\n",
    "credit_card_data.dropna(subset=['Total_Revolving_Bal','Months_Inactive_12_mon'],\n",
    "                   inplace=True\n",
    "                  )\n",
    "\n",
    "# Estraggo la  colonna delle label e la rimuovo dal dataset\n",
    "credit_card_label = credit_card_data['Attrition_Flag'].map(\n",
    "    {'Existing Customer':0,\n",
    "     'Attrited Customer':1\n",
    "    }\n",
    ").values\n",
    "credit_card_data.drop(columns=['Attrition_Flag',\n",
    "                               'CLIENTNUM',\n",
    "                               'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2',\n",
    "                               'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1'],\n",
    "                      inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unknown_imputer(X, missing_value = 'Unknown'):\n",
    "    X = X.values\n",
    "    unique_values, count = np.unique(X,return_counts=True)\n",
    "    num_nan = count[unique_values == missing_value]\n",
    "    counting = count[unique_values != missing_value]\n",
    "    values = unique_values[unique_values != missing_value]\n",
    "    X_new = X.copy()\n",
    "    freq = counting / np.sum(counting)\n",
    "    X_new[X_new == missing_value] = np.random.choice(values,size=num_nan,p=freq)\n",
    "    return X_new\n",
    "\n",
    "ui = FunctionTransformer(unknown_imputer)\n",
    "\n",
    "customer_age_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "category_pipeline = Pipeline([\n",
    "    ('imputer', FunctionTransformer(unknown_imputer)),\n",
    "    ('ordinal', OneHotEncoder())\n",
    "])\n",
    "\n",
    "features_robust = ['Credit_Limit','Total_Revolving_Bal','Avg_Open_To_Buy']\n",
    "features_standard = list(set(credit_card_data.select_dtypes(include=['int64','float64']).columns).difference(set(features_robust + ['Avg_Utilization_Ratio', 'Customer_Age'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessing = ColumnTransformer([\n",
    "    ('age', customer_age_pipeline, ['Customer_Age']),\n",
    "    ('gender', OrdinalEncoder(categories=[['M','F']]), ['Gender']),\n",
    "    ('edu', category_pipeline, ['Education_Level']),\n",
    "    ('status', category_pipeline, ['Marital_Status']),\n",
    "    ('income', category_pipeline, ['Income_Category']),\n",
    "    ('card', category_pipeline, ['Card_Category']),\n",
    "    ('numeric_robust', RobustScaler(), features_robust),\n",
    "    ('feature_standard', StandardScaler(), features_standard)\n",
    "],\n",
    "    remainder = 'passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = data_preprocessing.fit_transform(credit_card_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_name = ['Customer_Age','Gender']\n",
    "for c in ['Education_Level','Marital_Status','Income_Category']:\n",
    "    cat_inc_name = [c+'_cat{}'.format(i) for i in range(1,len(credit_card_data[c].unique()))]\n",
    "    columns_name.extend(cat_inc_name)\n",
    "columns_name.extend(['Card_Category_cat{}'.format(i) for i in range(1,len(credit_card_data['Card_Category'].unique())+1)])\n",
    "columns_name.extend(features_robust)\n",
    "columns_name.extend(features_standard)\n",
    "columns_name.append('Avg_Utilization_Ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificazione\n",
    "\n",
    "Ora siamo pronti per addestrare un modello ML e lo potremmo fare piuttosto facilmente grazie al fatto che abbiamo gia' preparato i dati e alla struttura di SKL.\n",
    "\n",
    "Iniziamo ad addestrare un Perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron = Perceptron()\n",
    "perceptron.fit(feature_matrix, credit_card_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo come si comporta con alcuni elementi del training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_training = feature_matrix[:10,:]\n",
    "sample_labels = credit_card_label[:10]\n",
    "prediction_sample = perceptron.predict(sample_training)\n",
    "for i in range(len(sample_labels)):\n",
    "    print(i,'True label: {} vs Predicted Label: {}'.format(sample_labels[i],prediction_sample[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un risultato che potrebbe sembrare incoraggiante...\n",
    "\n",
    "Proviamo a contare quante volte sbaglio su un set di dati gia' visto...\n",
    "\n",
    "E' fondamentale, infatti, determinare/stimare le performance del modello su un insieme di dati mai visto prima. In precedenza non abbiamo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predicted = perceptron.predict(feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(all_predicted == credit_card_label)/len(all_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non sembra male, ma ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predicted_dummy = np.zeros(len(credit_card_label))\n",
    "np.sum(all_predicted_dummy == credit_card_label)/len(all_predicted_dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il perceptron performance un po' meglio di un modello \"in tilt\",i.e un modello che predice sempre la classe piu' frequente.\n",
    "\n",
    "Questa piccola differenza di performance ci permette di introdurre alcune trappole in cui siamo caduti. \n",
    "1) La valutazione delle performance deve essere effettuata su dati non ancora 'visti' dall'algoritmo di apprendimento (errore metodologico)\n",
    "2) La valutazione deve tenere conto del bilanciamento o meno delle classi\n",
    "\n",
    "In questa fase, esploriamo il **primo** problema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "Le metodologie di cross-validation possono essere utili per ottenere delle valutazioni piu' attendibili circa le proprieta' di generalizzazione del modello appreso, cioe' quanto il modello sara' in grado di predirre correttamente la classe di appartenenza su dati non ancora osservati.\n",
    "\n",
    "Per prevenire il primo errore, e  di conseguenza l'overfitting dei dati, una pratica comune e' **holdout cross-validation**. Si divide il dataset in due insiemi distinti: training e test dataset. Il primo e' usato per l'addestramento del modello, il secondo per la valutazione delle performance e la valutazione delle capacita' di generalizzazione.\n",
    "\n",
    "Un raffinamento del precedente schema e' l'utilizzo di un terzo insieme: **validation set**, grazie al quale viene fatto una model selection, i.e. vengono identificati l'insieme degli iperparametri ottimali. <br>\n",
    "In questo modo ho il vantaggio di non utilizzare il test set nella fase di learning riducendo il bias sulla stima della capacita' di generalizzare su nuovi dati.\n",
    "\n",
    "Il modo in cui gli insiemi vengono partizionati influenza la stima delle performance.<br>\n",
    "Per aumentare la robustezza della stima possiamo utilizzare **k-fold cross validation**.\n",
    "\n",
    "Prima, pero', vediamo come fare holdout cross-validation in SKL. SKL mette a disposizione il metodo **train_test_split** che mi restituisce una partizione in training e test sia della matrice delle feature sia del vettore delle label. E' possibile specificare la frazione di elementi che costituiscono i due insiemi.\n",
    "\n",
    "Nel nostro caso, il training set e' costituito dal 70% del campione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(feature_matrix, credit_card_label, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron.fit(feature_matrix, credit_card_label)\n",
    "predicted_test = perceptron.predict(X_test)\n",
    "np.sum(predicted_test == y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nella k-fold cross-validatin - k-fold CV - il training set viene suddiviso in $k$ sottoinsiemi disgiunti - *fold* , dove ogni fold:\n",
    "1) i rimanenti $k-1$ fold sono utilizzati come training set\n",
    "2) il modello addestrato sui $k-1$ fold viene valutato sulla parte rimanente del dataset.\n",
    "\n",
    "In questo modo si ottengono $k$ modelli e $k$ misure di performance diverse. Si calcola, quindi una media delle performance. Una volta selezionato l'insieme degli iperparametri migliori, si riaddestra il modello sul training set completo e si effettua la valutazione del test set.\n",
    "\n",
    "![](kfold-cross_validation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attraverso il metodo **cross_val_score** possiamo usare direttamente k-CV e ottenere le misure di performance per ogni fold.\n",
    "\n",
    "Solitamente come valore da assegnare a $k$ si usa 5 o 10, a seconda della dimensione del dataset. In SKL, se $K$ e' un intero vengono utilizzate le classi **KFold** o **StratifiedKFold**, come strategie di partizionamento di default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron_score = cross_val_score(perceptron, X_train, y_train, cv = 10)\n",
    "perceptron_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Accuracy media: {:0.3f}'.format(np.mean(perceptron_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approfondiremo le misure di performance in seguito.\n",
    "\n",
    "Come vedremo il parametro **cv** ammette tipo di dato diverso, in modo da specificare quale strategia di divisione del set utilizzare in forma di classi SKL o iterable su un insieme di indici."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una funzione simile a *cross_val_score* e' **cross_val_predict**. Restituisce per ogni elemento nel training set, la label predetta quando quell'elemento era nel fold di test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_in_fold = cross_val_predict(perceptron, X_train, y_train, cv = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation Itertors\n",
    "La generazione delle fold dipende fortemente dal meto di partizionamento/campionamento adottato e dalle assunzioni sul tipo di dato presente nel dataset.\n",
    "\n",
    "Se assumiamo che i dati siano i.i.d.  - indipendenti e identicamente distribuiti - il processo generativo dei dati e' uno e non ha memoria dei dati generati in passato. In questo caso possiamo utilizzare:\n",
    "- k-fold\n",
    "- Repeated k-fold\n",
    "- LOO\n",
    "- LPO\n",
    "- Shuffle and Split\n",
    "\n",
    "La classe **KFold** implementa il k-fold splitting della k-fold CV. Vedi https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits = 10)\n",
    "perceptron_score = cross_val_score(perceptron, X_train, y_train, cv = kf)\n",
    "perceptron_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediante il metodo **split** posso ottenere l'insieme dei fold di training e il fold di test, espressi come indici della feature matrix. In sostanza indica quali righe inserire nel fold di training e quali inseire nel fold di test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train, test in kf.split(X_train, y_train):\n",
    "    print(X_train[train,:].shape, y_train[test].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo notare come nell'esempio precedente sul perceptron, i valori di performance possono oscillare ed influenzare la performance media. \n",
    "\n",
    "Una soluzione per ridurre l'effetto delle oscillazioni nel calcolo della performance media e' ripetere la k-fold CV per diverse volte e riportare la performance media calcolata su tutti i fold di tutte le ripetizioni. Ovviamente, il partizionamento tra due ripetizioni non e' lo stesso.<br>\n",
    "Questo approccio e' utile con dataset di piccola/media dimensione e che non richiedono un elevato costo di computazione nella fase di training, anche se e' facile parallelizzare il processo (SKL lo fa di default)\n",
    "\n",
    "SKL fornisce un'implementazione di tale metodo mediante la classe **RepeatedKFold** (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedKFold.html#sklearn.model_selection.RepeatedKFold). Si devono specificare i valori di $K$ e il numero di ripetizioni *n_repeats*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rkf = RepeatedKFold(n_splits = 10, n_repeats=3)\n",
    "perceptron_score = cross_val_score(perceptron, X_train, y_train, cv = rkf, n_jobs=-1)\n",
    "perceptron_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da notare il parametro *n_jobs* tramite cui specifico quanti core utilizzare in parallelo per eseguire CV.\n",
    "\n",
    "E' possibile anche valutare l'effetto del numero di ripetizioni sulle performance medie del classificatore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valuta_modello_ripetizione(X, y, repeats):\n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=repeats, random_state=1)\n",
    "    model = Perceptron()\n",
    "    scores = cross_val_score(model, X, y, cv=cv, n_jobs=-1)\n",
    "    return scores\n",
    "\n",
    "repeats = range(1,16)\n",
    "results = list()\n",
    "for r in repeats:\n",
    "    scores = valuta_modello_ripetizione(feature_matrix, credit_card_label, r)\n",
    "    print('>{} mean={:.4f} se={:.3f}'.format(r, np.mean(scores), sem(scores)))\n",
    "    results.append(scores)\n",
    "# plot the results\n",
    "plt.boxplot(results, labels=[str(r) for r in repeats], showmeans=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sempre nel setting iid, possiamo generare un insieme di split training/test in cui viene violata la condizione di insieme disgiunto tra le fold. In questo caso appliciamo una random permutation CV.\n",
    "\n",
    "In SKL tale strategia e' fornita dalla classe **ShuffleSplit**. Per ogni iterazione - splitting, la feature matrix viene permutata e una sua frazione di righe viene assegnata al test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spf = ShuffleSplit(n_splits = 10, test_size = 0.25)\n",
    "perceptron_score = cross_val_score(perceptron, X_train, y_train, cv = rkf, n_jobs=-1)\n",
    "perceptron_score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questo modo si ha un controllo maggiore sulla porzione di elementi nel test e si perde la dipendenza tra $k$ e *test_size*\n",
    "\n",
    "#### Stratification\n",
    "In alcuna situazione assistiamo - come nel nostro caso - ad un forrte sbilanciamento nella distribuzione delle classi nel vettore delle label. In questi casi si raccomanda di utilizzare un campionamento stratitificato implementato dalle classi SKL **StratifiedKFold** e **StratifiedShuffleSplit**. Tali classi assicurano che la distribuzione delle etichette in ogni fold approssimi quella dell'intero vettore delle etichette\n",
    "\n",
    "Nel caso di StratifiedKFold avremo la suddivisione in figura:\n",
    "![](stratified_kfold_cv.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10)\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "print('----K-fold CV----------')\n",
    "for train, test in kf.split(feature_matrix, credit_card_label):\n",
    "    y_train, y_test = credit_card_label[train], credit_card_label[test]\n",
    "    print('Train: {:.2f} vs Test: {:.2f}'.format(sum(y_train == 1)/len(y_train), sum(y_test == 1)/len(y_test)))\n",
    "print('----Stratified K-fold CV-------')\n",
    "for train, test in skf.split(feature_matrix, credit_card_label):\n",
    "    y_train, y_test = credit_card_label[train], credit_card_label[test]\n",
    "    print('Train: {:.2f} vs Test: {:.2f}'.format(sum(y_train == 1)/len(y_train), sum(y_test == 1)/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo stesso principio si applica alla metodologia di shuffle and split:\n",
    "![](stratified_shufflesplit_cv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Measure for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viste le diverse strategie di cross-validation, affrontiamo un secondo argomento essenziale: le misure di performance.\n",
    "\n",
    "Abbiamo gia' visto che il perceptron utilizzato in precedenza non era meglio di un classificatore che prediceva sempre l'etichetta piu' frequente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_dumb_cls = DummyClassifier(strategy='prior')\n",
    "uni_dumb_cls = DummyClassifier(strategy='uniform')\n",
    "st_dumb_cls = DummyClassifier(strategy='stratified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pappagallo_score = cross_val_score(mf_dumb_cls, X_train, y_train, cv=10, scoring='accuracy')\n",
    "pappagallo_score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cio' mostra come l'accuracy non e' una buona misura di performance, specialmente quando il vettore delle etichette e' sbilanciato.\n",
    "\n",
    "Un modo migliore per valutare le performance di un classificatore e' la **confusion matrix** $C$. L'elemento $C_{ij}$ della matrice corrisponde al numero di osservazioni che hanno etichetta $i$ e che sono state classificate con etichetta $j$. Nel caso di classificazione binario:\n",
    "1) $C_{00}$ = veri negativi\n",
    "2) $C_{01}$ = falsi positivi\n",
    "3) $C_{10}$ = falsi negativi\n",
    "4) $C_{11}$ = veri positivi\n",
    "\n",
    "Per calcolare la confusion matrix, necessitiamo di:\n",
    "1) il vettore delle etichette\n",
    "2) il vettore delle etichette predette\n",
    "Per ottenere il secondo elemento possiamo utilizzare il metodo  *cross_val_predict*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predicted = cross_val_predict(perceptron, X_train, y_train, cv = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infine per calcolare la confusion matrix utilizziamo il metodo **confusion matrix**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_train, y_train_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ogni riga corrisponde ad una classe _attuale_, mentre ogni colonna una _classe predetta_.\n",
    "\n",
    "Dalla matrice di confusione posso calcolare l'accuratezza delle predizioni positive, i.e. la **precision**:\n",
    "$$precision = \\frac{C_{11}}{C_{11} + C_{01}} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "a cui si accompagna un'altra metrica: la **recall** o **sensitivity** o **true positive rate**:\n",
    "$$precision = \\frac{C_{11}}{C_{11} + C_{10}} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "In SKL precision e recall sono restituite dai metodi **precision_score** e **recall_score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_train, y_train_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_train, y_train_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo le differenze confrontandoci con i due dummy classifier definiti in precedenza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_mfdumb = cross_val_predict(mf_dumb_cls, X_train, y_train, cv = 10)\n",
    "y_train_unidumb = cross_val_predict(uni_dumb_cls, X_train, y_train, cv = 10)\n",
    "y_train_stdumb = cross_val_predict(st_dumb_cls, X_train, y_train, cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_train,y_train_mfdumb), precision_score(y_train, y_train_unidumb), precision_score(y_train, y_train_stdumb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nel caso di dummy che predice la singola classe piu' frequente, la colonna nella confusion matrix relativa alla classe 1 e' composta da 0 => 0/0. Nei rimanenti casi, si puo' intuire il risultato se pensiamo al processo di classificazione implementato come una selezione. Nel caso predica 1, seleziono l'elemento del campione e mi chiedo quale sia la sua etichetta. In entrambi i casi vengono rispettate le proporzioni della classe nel campione originario. In un caso seleziono meno elementi (stratified), perche' le etichette sono sbilanciate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_train,y_train_mfdumb), recall_score(y_train, y_train_unidumb), recall_score(y_train, y_train_stdumb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nel primo caso la recall e' 0 perche' $C_{11}=0$. Per spiegare i rimanenti risultati, si selezionano i campioni con etichetta vera = 1, nel caso uniforme ottengo il 50% di elementi con etichetta 1, nel caso stratified ottengo solo il 16% con etichetta 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_train, y_train_unidumb), confusion_matrix(y_train, y_train_stdumb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sia in recall che precision, il perceptron si comporta leggermente meglio di un random guesser.\n",
    "\n",
    "In generale *una soluzione basata su perceptron non e' affidabile nel caso mi segnali un cliente 'in uscita', e neanche in grado di identificare un cliente 'in uscita'*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per avere un modo semplice per confrontare due classificatori, viene spesso impiegato l'indicatore $F_{1}$, definito come la media armonica tra precision e recall. Penalizza un classificatore con precision e recall sbilanciate.\n",
    "$F_1 = \\frac{2}{\\frac{1}{precision}+\\frac{1}{recall}}$. Questo indicatore viene implementato dal metodo **f1_score**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision/Recall Tradeoff\n",
    "Utilizziamo ora un logit classifier per il problema dell'identificazione dei churner dal momento che restituisce anche un prediction score. Nel caso del classificatore in questione, corrisponde al valore prima della funzione di threshold, i.e. $\\phi(\\mathbf{w}^T\\mathbf{x})$. Esso indica la confidenza sulla classe predetta. In SKL un logit classifier e' implementato dalla classe **LogisticRegression**, il quale rende disponibile il metodo **decision_function** per ottenere gli score di confidenza associati ad ogni istanza predetta.\n",
    "\n",
    "In questo modo possiamo agire sulla funzione di soglia e modificare la predizione finale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_cls = LogisticRegression(max_iter=1000)\n",
    "y_scores = cross_val_predict(logit_cls, X_train, y_train, cv = 10, method='decision_function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dato gli score, possiamo utilizzare il metodo **precision_recall_curve** per calcolare precision e recall al variare del valori di soglia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec, rec, soglia = precision_recall_curve(y_train, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,9))\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(soglia, prec[:-1], 'r',label='precision')\n",
    "ax.plot(soglia, rec[:-1], 'b',label='recall')\n",
    "ax.legend(fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oppure posso visualizzare la precision in funzione della recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,9))\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(rec[:-1], prec[:-1], 'r',label='precision', lw=5)\n",
    "ax.set_xlabel('recall')\n",
    "ax.set_ylabel('precision')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questo tipo di analisi ci permette di scegliere il valore in base alle nostre esigenze. Per esempio se volessimo aumentare la fino a 0.9 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soglia_prec_90 = soglia[np.argmax(prec >= 0.9)]\n",
    "y_train_prec_90 = y_scores >= soglia_prec_90\n",
    "precision_score(y_train, y_train_prec_90), f1_score(y_train, y_train_prec_90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve\n",
    "La curva **Receiver operating characteristic - ROC** e' molto simile alla curva precision/recall ma visualizza la recall in funzione del **false positive rate**:\n",
    "$$FPR = \\frac{C_{01}}{C_{01}+C{00}} = \\frac{FP}{FP+TN} = 1 - specificity$$\n",
    "Per visualizzare la curva ROC applichiamo lo stesso procedimento precedente utilizzo il metodo **roc_curve**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, rec, soglia = roc_curve(y_train, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,9))\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(fpr, rec, 'r',label='precision', lw=5)\n",
    "ax.plot()\n",
    "ax.set_xlabel('false positive rate')\n",
    "ax.set_ylabel('recall')\n",
    "ax.plot([0,1],[0,1],'k--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All'aumentare della recall, aumenta la produzione di falsi positivi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
