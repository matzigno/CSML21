{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import reciprocal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#Dataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks\n",
    "\n",
    "L'introduzione delle reti neurali artificiali e' datata al 1943, anno in cui McCulloch e Pitts presentano il primo modello computazionale ispirato al neurone.<br>\n",
    "Attualmente hanno guadagnato notevole interesse per diversi motivi contigenti:\n",
    "- enormi quantita' di dati per l'addestramento\n",
    "- aumento della capacita' computazionale per supportare il training di grandi ANN. Da sottolineare il contributo dall'industria del gaming con l'introduzione di GPU - graphical processing unit - e lo sviluppo di architetture ad hoc come TPU, FPGA o chip neuromorfici.\n",
    "- progressi negli algoritmi di apprendimento ed ottimizzazione\n",
    "\n",
    "Il perceptron e' la piu' semplice architettura ANN ed e' basato su una singola linear threshold unit - LTU.\n",
    "$$y_{\\mathbf{w}}(\\mathbf{x}) = \\phi(\\mathbf{w}^T\\mathbf{x})$$\n",
    "dove $\\phi$ puo' essere una funzione di Heaviside oppure una funzione segno:\n",
    "$$ hs(x) = \n",
    "\\begin{cases}\n",
    "    0 & \\text{se } x<0\\\\\n",
    "    1 & \\text{se} x \\geq 0\n",
    "\\end{cases}\n",
    "\\hspace{1cm}\n",
    "sgn(x) = \n",
    "\\begin{cases}\n",
    "    -1 & \\text{se } x<0\\\\\n",
    "    0 & \\text{se} x == 0\\\\\n",
    "    1 & \\text{se} x\\geq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "In general un perceptron puo' essere composto da una sequenza di LTU - **layer** - dove ogni LTU di un layer e' connessa ad ogni LTU del layer precedente. In questo caso si parla di **fully connected** o **dense layer**. \n",
    "![](perceptron_layer.png)\n",
    "\n",
    "Nella figura vediamo come il bias venga inserito nell'**input layer** come unita' con valore 1.\n",
    "\n",
    "E' possibile formulare la seguente architettura in termini matriciali, considerando l'intero feature matrix o matrice delle istanze $\\mathbf{X}$:\n",
    "$$y_{\\mathbf{W,b}}(\\mathbf{X}) = \\phi(\\mathbf{XW} + \\mathbf{b})$$\n",
    "\n",
    "dove $\\phi$ indica una generica **activation function**.\n",
    "\n",
    "La fase di learning del perceptron utilizza il gradiente discendente, il quale e' ispirato a *Hebbian learning* - la forza della connessione tra due neuroni aumenta quando uno dei due \"innesca\" o \"stimola\" il secondo. Nel caos del perceptron vengono rafforzate i pesi delle connessioni che riducono l'errore di predizione.\n",
    "\n",
    "Sulla base dell'architettura del perceptron si possono creare architetture piu' complesse \"impilando\" diversi dense layers; definendo quindi un **Multilayer Perceptron - MLP**.\n",
    "![](mlp.png)\n",
    "\n",
    "L'elemento caratterizzante questa architettura e' la presenza di 1 o piu' livelli nascosti - **hidden layers**. Ogni hidden layer include anche un'unita di bias.<br>\n",
    "Quando gli hidden layer sono molti si parla di architettura **deep neural network - DNN**. \n",
    "\n",
    "L'utilizzo di un'architettura deep e' resa computazionalmente ammissibile dall'algoritmo di backpropagation (1986) che permette di aggiornare i pesi di tutti i livelli mediante due scansioni della rete: una forward e una backward secondo un approccio di tipo Gradient Descent.\n",
    "\n",
    "Rispetto alla LTU del perceptron viene introdotta un'ulteriore modifica per evitare che il gradiente venga posto a 0 - heaviside e sgn hanno gradiente 0. Le alternative per definire la funzione di attivazione $\\phi$ sono molteplici, tuttavia in pratica vengono utilizzate prevalentemente 3 funzioni:\n",
    "- sigmoid: $\\phi(x) = 1 / (1 + exp(-x))$\n",
    "- hyperbolic tangent: $\\phi(x) = tanh(x)$\n",
    "- ReLU - Rectified Linear Unit: $\\phi(x) = max(0,x)$. Non e' differenziabile in 0 ma e' diventata lo standard de facto per performance e facilita' di computazione.\n",
    "\n",
    "Le activation funciont sono fondamentali nelle architetture DNN perche' introducono la non-linearita'; senza activation function non lineari la composizione dei layer restituirebbe una funzione lineare in quanto ogni layer e' una funzione lineare $\\mathbf{w}^T\\mathbf{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP for Regression\n",
    "Per applicare MLP ad un problema di regressione necessitiamo di una singola unita' di output che restituisce un valore reale, il valore predetto. E' possibile aggiungere altre unita' di output di tipo reale in caso di regressione multivariata - predico piu' valori associati ad una nuova istanza. <br>\n",
    "Nel caso di regressione l'activation function non e' necessaria tuttavia, posso utilizzare alcune funzioni se voglio garantire alcune condizioni sull'output:\n",
    "- output positivo: ReLU, softplus $\\phi(x)=log(1+exp(x))$\n",
    "- insieme limitato: logistic o $tanh$\n",
    "Come loss function si possono utilizzare MSE - mean squared error, MAE - mean absolute error oppure Huber loss.\n",
    "\n",
    "### MLP for Classification\n",
    "Per un problema di classificazione binaria e' necessaria una sola unita' di output tra 0 e 1 - probabilita' della classe positiva. Per gestire un problema di multiclass classification viene inserita un'unita' di output per ogni classe a cui applicare una funzione di attivazione softmax. Se $\\mathbf{s}(\\mathbf{x})$ e' il vettore che in posizione $k$ contiene lo score assegnato alla classe $k$, cioe' output della $k$-esima unita' di output, la softmax function di $\\mathbf{s}_k(\\mathbf{x})$ e' data da:\n",
    "$$\\sigma(\\mathbf{s}(\\mathbf{x})_k))=\\frac{exp(\\mathbf{s}_k(\\mathbf{x}))}{\\sum_{j=1}^K exp(\\mathbf{s}_j(\\mathbf{x}))}$$\n",
    "e la classe predetta corrisonde al valore di $k$ con softmax function piu' elevata.<br>\n",
    "Come loss function si utilizza la cross-entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing MLPs with Keras\n",
    "Keras (https://keras.io/) e' un API di alto livello per il deep learning che permette di definire, addestrare e valutare qualsiasi architettura di tipo ANN.\n",
    "\n",
    "Keras si basa su un backend per la computazione che puo' essere TensorFlow, CNTK o Theano. Tuttavia, TensorFlow viene rilasciato con un'implementazione di Keras a cui si aggiungono delle funzionalita' extra.\n",
    "\n",
    "In questo notebook utilizzeremo l'implementazione delle API di Keras rilasciata in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come esempio applicativo utilizziamo un dataset simile al MNIST, ma relativo ad immagini di capi di abbigliamento a cui e' stata assegnata una label indicante il tipo di capo. Affrontiamo quindi un problema di classificazione multiclasse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 1us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 3s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0s/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 1s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (10000, 28, 28))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train_full, y_train_full),(X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Su questo dataset, estraiamo un validation set dal training set e rilascaliamo le immagini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((55000, 28, 28), (5000, 28, 28))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Sneaker'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.imshow(X_train[2], cmap=\"binary\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "class_names[y_train[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creazione del modello MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abbiamo creato un modello di tipo **Sequential** - uno dei piu' semplici - in cui i livelli sono messi in sequenza. Stiamo quindi impilando gli hidden layer secondo l'architettura MLP. Il primo livello - quello di input - e' modellato da un oggetto **Flatten** che converte l'input originario (matrice 28x28) in un vettore, linearizzando o flattenizzando la matrice dell'immagine. Attraverso il parametro **input_shape** specifico la dimensionalita' dell'input, cioe' il numero di feature di ogni istanza.\n",
    "\n",
    "Viene aggiunto un hidden layer di tipo fully connected mediante la creazione di un oggetto **Dense**. Devo specificare quante unita' compongono il livello e quale funzione di attivazione utilizzare.\n",
    "\n",
    "Dopo aver aggiunto un ulteriore hidden layer contenente 100 unita' e funzione di attivazione ReLU, viene aggiunto il livello di output. Dal momento che il numero di classi e' 10 e siamo in un contesto di multiclass classification posso utilizzare softmax come funzione di attivazione, in modo da ottenere un singolo input.\n",
    "\n",
    "NB: Ogni livello gestisce la sua matrice dei pesi $\\mathbf{W}$ e il vettore dei bias $\\mathbf{b}$. Tali elementi possono essere ottenuti mediante il metodo **get_weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_h1, b_h1 = model.layers[1].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.02966851,  0.06497736, -0.03102705, ...,  0.02661753,\n",
       "         -0.03735466,  0.05685103],\n",
       "        [-0.00971091, -0.02967479,  0.01740143, ...,  0.01935496,\n",
       "         -0.00887664,  0.067968  ],\n",
       "        [ 0.02080519, -0.0449068 ,  0.06666592, ...,  0.05642565,\n",
       "         -0.04088403, -0.05875728],\n",
       "        ...,\n",
       "        [-0.00314967, -0.03852612, -0.05186538, ..., -0.07165267,\n",
       "         -0.05047796, -0.03949234],\n",
       "        [-0.06030037,  0.03175658,  0.00421286, ..., -0.03754115,\n",
       "         -0.06017287, -0.00452667],\n",
       "        [-0.07061649, -0.00258705, -0.03754215, ...,  0.06227942,\n",
       "         -0.04031346, -0.00485733]], dtype=float32),\n",
       " (784, 300),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " (300,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_h1, W_h1.shape, b_h1, b_h1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matrice dei pesi di ogni layer viene inizializzata in modo casuale, mentre il vettore dei bias viene inizializzato con 0. E' possibile modificare questo comportamento tramite gli argomenti **set_kernel_initializer** e **bias_initializer**.\n",
    "\n",
    "\n",
    "Il metodo **summary** visualizza tutti i livelli del modello e fornisce alcune informazioni sulla dimensione dei livelli e il numero di parametri del modello. Mentre l'attributo **layers** permettere di accedere ad un singolo livello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il modello ha 266,610 parametri, e' flessibile ma incline all'overfitting. Abbiamo bisogno di avere molti dati di training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilazione del modello\n",
    "Dopo la creazione di un modello devo specificare la funzione di loss e l'algoritmo di ottimizzazione, ed eventualmente le metriche di performance da utilizzare. Il metodo **compile** viene invocato per questi scopi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'sparse_categorical_crossentropy',\n",
    "              optimizer = 'sgd',\n",
    "              metrics =  ['accuracy']\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Il training del modello avviene invocando il metodo **fit**. Richiede il training set e il relativo label vector, il numero di epoche (quante volte devo scansionare tutto il training set). Posso passare anche un validation set in modo da verificare come il modello si comporta su dati \"nuovi\" attraverso l'argomento **validation_data**.\n",
    "\n",
    "Per definire un validation set posso utilizzare il parametro **validation_split** che indica la percentuale di istanze da inserire nel validation set. \n",
    "\n",
    "Nel caso la distribuzione delle classi non sia bilanciata possiamo applicare un vettore di pesi alle classi in modo da dare piu' peso alle classi sottorappresentate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 28, 28) (5000, 28, 28)\n",
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.7076 - accuracy: 0.7675 - val_loss: 0.4990 - val_accuracy: 0.8338\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4883 - accuracy: 0.8302 - val_loss: 0.4522 - val_accuracy: 0.8470\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4441 - accuracy: 0.8442 - val_loss: 0.4361 - val_accuracy: 0.8452\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4179 - accuracy: 0.8541 - val_loss: 0.4309 - val_accuracy: 0.8420\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3978 - accuracy: 0.8601 - val_loss: 0.3871 - val_accuracy: 0.8656\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3821 - accuracy: 0.8646 - val_loss: 0.3689 - val_accuracy: 0.8686\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3680 - accuracy: 0.8693 - val_loss: 0.3600 - val_accuracy: 0.8728\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3549 - accuracy: 0.8729 - val_loss: 0.3823 - val_accuracy: 0.8602\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3461 - accuracy: 0.8762 - val_loss: 0.3803 - val_accuracy: 0.8648\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3359 - accuracy: 0.8791 - val_loss: 0.3457 - val_accuracy: 0.8776\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3275 - accuracy: 0.8821 - val_loss: 0.3341 - val_accuracy: 0.8802\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3191 - accuracy: 0.8850 - val_loss: 0.3403 - val_accuracy: 0.8802\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3117 - accuracy: 0.8878 - val_loss: 0.3321 - val_accuracy: 0.8844\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3054 - accuracy: 0.8892 - val_loss: 0.3261 - val_accuracy: 0.8806\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2988 - accuracy: 0.8915 - val_loss: 0.3238 - val_accuracy: 0.8852\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2924 - accuracy: 0.8943 - val_loss: 0.3444 - val_accuracy: 0.8768\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2875 - accuracy: 0.8954 - val_loss: 0.3216 - val_accuracy: 0.8854\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2811 - accuracy: 0.8990 - val_loss: 0.3124 - val_accuracy: 0.8908\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2763 - accuracy: 0.8997 - val_loss: 0.3186 - val_accuracy: 0.8886\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2715 - accuracy: 0.9027 - val_loss: 0.3101 - val_accuracy: 0.8870\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2667 - accuracy: 0.9034 - val_loss: 0.3177 - val_accuracy: 0.8860\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2615 - accuracy: 0.9049 - val_loss: 0.3079 - val_accuracy: 0.8924\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2585 - accuracy: 0.9065 - val_loss: 0.3041 - val_accuracy: 0.8928\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.2529 - accuracy: 0.9083 - val_loss: 0.3396 - val_accuracy: 0.8746\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.2490 - accuracy: 0.9096 - val_loss: 0.3250 - val_accuracy: 0.8846\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.2457 - accuracy: 0.9104 - val_loss: 0.3149 - val_accuracy: 0.8860\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2405 - accuracy: 0.9126 - val_loss: 0.2943 - val_accuracy: 0.8926\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2373 - accuracy: 0.9136 - val_loss: 0.3028 - val_accuracy: 0.8900\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2328 - accuracy: 0.9156 - val_loss: 0.3232 - val_accuracy: 0.8868\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2297 - accuracy: 0.9167 - val_loss: 0.2971 - val_accuracy: 0.8956\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_valid.shape)\n",
    "history = model.fit(X_train, y_train,\n",
    "          epochs=30,\n",
    "          validation_data=(X_valid,y_valid)\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'oggetto **History** restituito dal metodo fit contiene i parametri di training (**.params**), la lista delle epoche (**.epoch**) e un dict con i valori della loss function e delle metriche di performance sul training e validation set per ogni epoca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'verbose': 1, 'epochs': 30, 'steps': 1719},\n",
       " [0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.params, history.epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(12,4))\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Osserviamo un po' di overfitting.\n",
    "\n",
    "Nel caso venga invocato nuovamente il metodo fit, senza ricreare il modello, il processo di training riprende dall'ultimo stato dei parametri.\n",
    "\n",
    "Per valutare il modello appresso sul test set utilizziamo il metodo **evaluate**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 84.9280 - accuracy: 0.8212\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[84.92796325683594, 0.8212000131607056]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediante il metodo **predict** eseguiamo una predizione circa una nuova istanza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se invece voglio ottenere la classe predetta, si utilizza il metodo **predict_classes**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-21-d2054f4546e8>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U11')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(class_names)[model.predict_classes(X_test[:3])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visto che il metodo  deprecato, seguiamo il consiglio del warning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U11')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(class_names)[np.argmax(model.predict(X_test[:3]),axis=-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression by Sequential API\n",
    "Affrontiamo un problema di regressione utilizzando un'architettura simile.\n",
    "\n",
    "Utilizziamo un dataset in cui siamo interessati a predirre il prezzo di real estate in California"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La differenza principale con l'architettura precedente  la presenza di una singola unit di output che non utilizza una funzione di attivazione. Come loss function utilizziamo MSE - mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.8096 - val_loss: 0.9276\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.8117 - val_loss: 0.7210\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.7064 - val_loss: 0.6624\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6611 - val_loss: 0.6486\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.6283 - val_loss: 0.6064\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6017 - val_loss: 0.5780\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5796 - val_loss: 0.5464\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5606 - val_loss: 0.5918\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5450 - val_loss: 0.5618\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5312 - val_loss: 0.4960\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5192 - val_loss: 0.5154\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5085 - val_loss: 0.4946\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4993 - val_loss: 0.4659\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4907 - val_loss: 0.4703\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4830 - val_loss: 0.4501\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4763 - val_loss: 0.4440\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4699 - val_loss: 0.4400\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4645 - val_loss: 0.4371\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4593 - val_loss: 0.4282\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4545 - val_loss: 0.4227\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 0.4486\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3)\n",
    "             )\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=20,\n",
    "                    validation_data=(X_valid, y_valid)\n",
    "                   )\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:3]\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4897409],\n",
       "       [1.5405464],\n",
       "       [3.2732165]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.477  , 0.458  , 5.00001])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex Models - Functional API\n",
    "Per costruire modelli con topologie pi complesse si utilizza **FunctionalAPI**. <br>\n",
    "Si vuole definire un architettura di tipo *Wide and Deep* mostrata in figura:\n",
    "\n",
    "![](wide_deep_1.png)\n",
    "\n",
    "Una parte o tutto il layer di input  connesso direttamente con il layer di output e, allo stesso tempo, il layer di input o parte di esso  alla base di un'architettura MLP deep. In questo modo  possibile apprendere dei pattern pi complessi e delle regole pi semplici nello stesso momento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.models.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In primis, definiamo l'input layer specificando la dimensione di una singola istanza di input: **shape** e **dtype**. Poi definiamo un hidden layer denso passando il livello di input come argomento di una funzione. In questo modo definiamo una dipendenza funzionale tra hidden layer e input layer ed indichiamo a Keras che i due livelli devono essere connessi.<br>\n",
    "L'elemento di novit  dato dal layer **concatenate**. Questo livello viene generato concatenando i livelli presi come argomento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 2.3975 - val_loss: 1.0799\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.7180 - val_loss: 0.6454\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6521 - val_loss: 0.6076\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6162 - val_loss: 0.5626\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5865 - val_loss: 0.5490\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5609 - val_loss: 0.5761\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5410 - val_loss: 0.5200\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5229 - val_loss: 0.4855\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5072 - val_loss: 0.4663\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4938 - val_loss: 0.4677\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4819 - val_loss: 0.4542\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4715 - val_loss: 0.4614\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4625 - val_loss: 0.4365\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4543 - val_loss: 0.4191\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4464 - val_loss: 0.4278\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4399 - val_loss: 0.4125\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4337 - val_loss: 0.4044\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4283 - val_loss: 0.3982\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - ETA: 0s - loss: 0.427 - 1s 2ms/step - loss: 0.4231 - val_loss: 0.4189\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4192 - val_loss: 0.3902\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 0.4127\n",
      "0.4127441346645355 [[0.60019404]\n",
      " [1.7839913 ]\n",
      " [3.2653577 ]]\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mean_squared_error\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3)\n",
    "             )\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=20,\n",
    "                    validation_data=(X_valid, y_valid)\n",
    "                   )\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "y_pred = model.predict(X_new)\n",
    "print(mse_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E' anche possibile utilizzare una parte dell'input nel ramo *wide* e una parte di input - anche sovrapposta con la prima - nel ramo *deep*. Come mostrato in figura.\n",
    "\n",
    "![](multi_in_mlp.png)\n",
    "\n",
    "\n",
    "Il precedente modello viene codificato nel seguente modo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"output\")(concat)\n",
    "model = keras.models.Model(inputs=[input_A, input_B], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La seguente definizione impatta sugli argomenti dei metodo fit, evaluate e predict, dal momento che dobbiamo passare un numero di sottomatrici della feature matrix pari al numero di livelli di input definiti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4149 - val_loss: 0.3873\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4132 - val_loss: 0.4084\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4117 - val_loss: 0.3866\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4102 - val_loss: 0.3831\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4087 - val_loss: 0.3803\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4072 - val_loss: 0.3813\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4060 - val_loss: 0.3792\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4047 - val_loss: 0.3800\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4036 - val_loss: 0.3858\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4022 - val_loss: 0.3954\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4015 - val_loss: 0.3755\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4000 - val_loss: 0.3774\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3991 - val_loss: 0.3730\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3983 - val_loss: 0.3912\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3971 - val_loss: 0.3836\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3964 - val_loss: 0.3700\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3954 - val_loss: 0.3839\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3942 - val_loss: 0.3977\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3940 - val_loss: 0.3690\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3930 - val_loss: 0.3725\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 0.3911\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024BBC658EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "0.39114031195640564 [[0.61686623]\n",
      " [2.0218587 ]\n",
      " [3.3672397 ]]\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", \n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3)\n",
    "             )\n",
    "\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "history = model.fit((X_train_A, X_train_B),\n",
    "                    y_train, epochs=20,\n",
    "                    validation_data=((X_valid_A, X_valid_B), y_valid))\n",
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "y_pred = model.predict((X_new_A, X_new_B))\n",
    "print(mse_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E' possibile anche definire pi output layers, nel caso di multitask classification oppure per inserire un fattore di regolarizzazione. La modalit di aggiunta di livelli di output multipli  analoga a quanto visto in precedenza. Per esempio se volessimo definire il modello mostrato in figura, in cui si applica una regolarizzazione sul ramo *deep*:\n",
    "\n",
    "![](multi_out_mlp.png)\n",
    "\n",
    "Otterremmo il seguente codice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"main_output\")(concat)\n",
    "aux_output = keras.layers.Dense(1, name=\"aux_output\")(hidden2)\n",
    "model = keras.models.Model(inputs=[input_A, input_B],\n",
    "                           outputs=[output, aux_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ogni output layers richiede una specifica loss function.<br>\n",
    "Posso priviligiare il contributo delle diverse loss function mediante il parametro **loss_weight** del metodo compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=[\"mse\", \"mse\"],\n",
    "              loss_weights=[0.9, 0.1], \n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3)\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anche in questo caso il numero di label vector passati deve essere uguale al numero di output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 2.5796 - main_output_loss: 2.4197 - aux_output_loss: 4.0191 - val_loss: 1.2634 - val_main_output_loss: 0.8343 - val_aux_output_loss: 5.1248\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.8628 - main_output_loss: 0.6842 - aux_output_loss: 2.4707 - val_loss: 1.0841 - val_main_output_loss: 0.6076 - val_aux_output_loss: 5.3723\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.7297 - main_output_loss: 0.5976 - aux_output_loss: 1.9182 - val_loss: 1.0156 - val_main_output_loss: 0.5601 - val_aux_output_loss: 5.1158\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6676 - main_output_loss: 0.5615 - aux_output_loss: 1.6227 - val_loss: 0.9797 - val_main_output_loss: 0.6096 - val_aux_output_loss: 4.3109\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6249 - main_output_loss: 0.5350 - aux_output_loss: 1.4340 - val_loss: 0.8900 - val_main_output_loss: 0.5110 - val_aux_output_loss: 4.3009\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5943 - main_output_loss: 0.5131 - aux_output_loss: 1.3246 - val_loss: 0.8323 - val_main_output_loss: 0.4945 - val_aux_output_loss: 3.8720\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5709 - main_output_loss: 0.4957 - aux_output_loss: 1.2478 - val_loss: 0.7853 - val_main_output_loss: 0.4802 - val_aux_output_loss: 3.5311\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5524 - main_output_loss: 0.4815 - aux_output_loss: 1.1904 - val_loss: 0.7837 - val_main_output_loss: 0.5536 - val_aux_output_loss: 2.8547\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5384 - main_output_loss: 0.4712 - aux_output_loss: 1.1433 - val_loss: 0.7028 - val_main_output_loss: 0.4566 - val_aux_output_loss: 2.9189\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5255 - main_output_loss: 0.4608 - aux_output_loss: 1.1085 - val_loss: 0.6671 - val_main_output_loss: 0.4430 - val_aux_output_loss: 2.6842\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5151 - main_output_loss: 0.4528 - aux_output_loss: 1.0758 - val_loss: 0.6369 - val_main_output_loss: 0.4267 - val_aux_output_loss: 2.5282\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5065 - main_output_loss: 0.4467 - aux_output_loss: 1.0455 - val_loss: 0.6123 - val_main_output_loss: 0.4178 - val_aux_output_loss: 2.3622\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4987 - main_output_loss: 0.4408 - aux_output_loss: 1.0197 - val_loss: 0.5891 - val_main_output_loss: 0.4285 - val_aux_output_loss: 2.0336\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4916 - main_output_loss: 0.4360 - aux_output_loss: 0.9923 - val_loss: 0.5666 - val_main_output_loss: 0.4152 - val_aux_output_loss: 1.9293\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4854 - main_output_loss: 0.4320 - aux_output_loss: 0.9656 - val_loss: 0.5523 - val_main_output_loss: 0.4029 - val_aux_output_loss: 1.8967\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4795 - main_output_loss: 0.4280 - aux_output_loss: 0.9434 - val_loss: 0.5396 - val_main_output_loss: 0.3983 - val_aux_output_loss: 1.8113\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4746 - main_output_loss: 0.4249 - aux_output_loss: 0.9213 - val_loss: 0.5209 - val_main_output_loss: 0.3989 - val_aux_output_loss: 1.6192\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4702 - main_output_loss: 0.4225 - aux_output_loss: 0.8996 - val_loss: 0.5101 - val_main_output_loss: 0.3940 - val_aux_output_loss: 1.5549\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4663 - main_output_loss: 0.4203 - aux_output_loss: 0.8803 - val_loss: 0.5021 - val_main_output_loss: 0.3910 - val_aux_output_loss: 1.5018\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4628 - main_output_loss: 0.4185 - aux_output_loss: 0.8615 - val_loss: 0.4926 - val_main_output_loss: 0.3896 - val_aux_output_loss: 1.4196\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train_A, X_train_B],\n",
    "                    [y_train, y_train],\n",
    "                    epochs=20,\n",
    "                    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid])\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 1ms/step - loss: 0.4568 - main_output_loss: 0.4151 - aux_output_loss: 0.8328\n",
      "WARNING:tensorflow:6 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024BBC38A3A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "0.4568482041358948 0.4150797128677368 0.8327642679214478\n"
     ]
    }
   ],
   "source": [
    "total_loss, main_loss, aux_loss = model.evaluate(\n",
    "    [X_test_A, X_test_B], [y_test, y_test])\n",
    "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])\n",
    "print(total_loss, main_loss, aux_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading a Model\n",
    "Il metodo **save** di Model o di Sequential permette di salvare tutte le componenti di un modello: archiettura, parametri del modello per ogni layer, iperparametri e il metodo di ottimizzazione utilizzato. Il formato utilizzato  HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"multiin_multiout_test.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per caricare un modello utilizzo il metodo **load_model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"multiin_multiout_test.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuttavia nel caso il processo di training duri molte ore  possibile eseguire dei salvataggi intermedi specificando al momento fit degli oggetti di callback.\n",
    "\n",
    "### Callbacks\n",
    "Il metodo fit accetta un argomento **callbacks** in cui  possibile una lista di oggetti che Keras invoca all'inizio o alla fine del training, all'inizio e/o alla fine di un epoca, all'inizio e/o alla fine di un batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"simple_mlp.h5\", save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'oggetto **ModelCheckpoint** salva il modello nel file specificato alla fine di ogni epoca. Nello specifico il salvataggio avviene solo se all'epoca attuale le performance sul validation set sono migliori delle epoche precedenti. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 2.2521 - val_loss: 0.9359\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.7073 - val_loss: 0.6239\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6027 - val_loss: 0.5588\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5586 - val_loss: 0.5098\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5264 - val_loss: 0.4853\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5024 - val_loss: 0.4650\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4838 - val_loss: 0.4598\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4681 - val_loss: 0.4415\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4551 - val_loss: 0.4506\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4449 - val_loss: 0.4446\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 0.4342\n",
      "0.4341915547847748\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3)\n",
    "             )\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb])\n",
    "\n",
    "model = keras.models.load_model(\"simple_mlp.h5\") #carico il modello che best performance su validation set\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "print(mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E' possibile implementare, mediante callback, un meccanismo di early stopping, utilizzando la classe **EarlyStopping**. Il processo di training viene interrotto quando non si misurano miglioramenti della loss function sul validation set per un numero specificato di epoche - paramtro **patience**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  1/363 [..............................] - ETA: 0s - loss: 0.2605WARNING:tensorflow:Callbacks method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_begin` time: 0.0010s). Check your callbacks.\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4342 - val_loss: 0.4382\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4260 - val_loss: 0.4108\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4199 - val_loss: 0.4191\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4147 - val_loss: 0.4283\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4109 - val_loss: 0.4086\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4059 - val_loss: 0.4008\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4039 - val_loss: 0.4022\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3999 - val_loss: 0.4163\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3979 - val_loss: 0.4067\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3953 - val_loss: 0.3904\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3918 - val_loss: 0.4201\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3898 - val_loss: 0.3968\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3877 - val_loss: 0.3799\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3857 - val_loss: 0.3737\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3839 - val_loss: 0.3849\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3828 - val_loss: 0.3770\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3809 - val_loss: 0.3973\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3790 - val_loss: 0.3854\n",
      "  1/162 [..............................] - ETA: 0s - loss: 0.3287WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0025s). Check your callbacks.\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 0.3858\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3)\n",
    "             )\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=4,\n",
    "                                                  restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_early = keras.models.load_model('simple_mlp.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121/121 [==============================] - 0s 1ms/step - loss: 0.3737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3736589848995209"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_early.evaluate(X_valid,y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard\n",
    "TensorBoard  uno strumento di visualizzazione che permette di osservare le propriet e le performance del modello definito e appreso in Keras/TensorFlow\n",
    "\n",
    "Per poter utilizzare tale strumento si devono creare dei file binari di log - **event file** - che TB analizza e visualizza\n",
    "\n",
    "Passo 1: Definire una directory che contiene i log per TB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_logdir(root_dir):\n",
    "    return os.path.join(root_dir, time.strftime(\"run_%Y_%m_%d-%H_%M_%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\tb_logs\\run_2021_02_04-07_03_48\n"
     ]
    }
   ],
   "source": [
    "logdir = get_run_logdir(os.path.join(os.curdir, \"tb_logs\"))\n",
    "print(logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras fornisce una classe callback **TensorBoard** che si occupa della scrittura degli event file nella directory specificata. Nel caso la directory non esistesse, viene creata.\n",
    "\n",
    "Ogni directory di log contiene due directory: una per il training set e una per il validation set, se utilizzato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  1/363 [..............................] - ETA: 0s - loss: 6.0900WARNING:tensorflow:From C:\\Users\\matte\\anaconda3\\envs\\CSML21\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "  2/363 [..............................] - ETA: 17s - loss: 6.0153WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0968s). Check your callbacks.\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 2.3651 - val_loss: 1.0782\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.8234 - val_loss: 0.6874\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6764 - val_loss: 0.6160\n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6176 - val_loss: 0.5876\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5758 - val_loss: 0.6176\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5440 - val_loss: 0.5710\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5181 - val_loss: 0.5291\n",
      "Epoch 8/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4967 - val_loss: 0.5127\n",
      "Epoch 9/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4803 - val_loss: 0.4619\n",
      "Epoch 10/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4665 - val_loss: 0.4400\n",
      "Epoch 11/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4551 - val_loss: 0.4261\n",
      "Epoch 12/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4453 - val_loss: 0.4201\n",
      "Epoch 13/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4377 - val_loss: 0.4087\n",
      "Epoch 14/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4306 - val_loss: 0.4035\n",
      "Epoch 15/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4247 - val_loss: 0.3976\n",
      "Epoch 16/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4197 - val_loss: 0.3922\n",
      "Epoch 17/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4149 - val_loss: 0.3883\n",
      "Epoch 18/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4108 - val_loss: 0.3833\n",
      "Epoch 19/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4069 - val_loss: 0.3804\n",
      "Epoch 20/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4037 - val_loss: 0.3801\n",
      "Epoch 21/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4004 - val_loss: 0.3762\n",
      "Epoch 22/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3974 - val_loss: 0.3745\n",
      "Epoch 23/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3949 - val_loss: 0.3711\n",
      "Epoch 24/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3922 - val_loss: 0.3722\n",
      "Epoch 25/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3899 - val_loss: 0.3740\n",
      "Epoch 26/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3878 - val_loss: 0.3682\n",
      "Epoch 27/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3856 - val_loss: 0.3617\n",
      "Epoch 28/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3835 - val_loss: 0.3673\n",
      "Epoch 29/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3816 - val_loss: 0.3600\n",
      "Epoch 30/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3798 - val_loss: 0.3576\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])    \n",
    "model.compile(loss=\"mse\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3)\n",
    "             )\n",
    "\n",
    "logdir = get_run_logdir(os.path.join(os.curdir, \"tb_logs\"))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(logdir)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passo 2: Eseguire TensorBoard.\n",
    "\n",
    "In Jupyter si deve caricare l'estensione tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Timed out waiting for TensorBoard to start. It may still be running as pid 12956."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./tb_logs --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Hyperparameters\n",
    "Nelle DNN il numero di iperparametri  elevato ed aumenta in modo proporzionale alla profondit dell'architettura.\n",
    "\n",
    "E' in ogni caso possibile utilizzare gli strumenti di ricerca degli iperparametri migliori forniti da SKL. Mediante l'oggetto **KerasRegressor** o **KerasClassifier**  possibile costruire un wrapper del modello definito in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.4683 - val_loss: 3.7223\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.6129 - val_loss: 1.6306\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5199 - val_loss: 0.6703\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4818 - val_loss: 0.4984\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4561 - val_loss: 0.4225\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4417 - val_loss: 0.4725\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4320 - val_loss: 0.4058\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4258 - val_loss: 0.4985\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4212 - val_loss: 0.3900\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4137 - val_loss: 0.4220\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4097 - val_loss: 0.4429\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4045 - val_loss: 0.3935\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4013 - val_loss: 0.4484\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3977 - val_loss: 0.3729\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3945 - val_loss: 0.4085\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3916 - val_loss: 0.4281\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3892 - val_loss: 0.3678\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3859 - val_loss: 0.4197\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3837 - val_loss: 0.4221\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3820 - val_loss: 0.4073\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3805 - val_loss: 0.3718\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3775 - val_loss: 0.3727\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3760 - val_loss: 0.3509\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3741 - val_loss: 0.3750\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3728 - val_loss: 0.4148\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3714 - val_loss: 0.3927\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3704 - val_loss: 0.3757\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3681 - val_loss: 0.3456\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3672 - val_loss: 0.3469\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3660 - val_loss: 0.5741\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3659 - val_loss: 0.3501\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3635 - val_loss: 0.4960\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3639 - val_loss: 0.5660\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3627 - val_loss: 0.8667\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3649 - val_loss: 0.4152\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3611 - val_loss: 0.8390\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3639 - val_loss: 0.3734\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3591 - val_loss: 0.8023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24ba26524f0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_reg.fit(X_train, y_train, epochs=100,\n",
    "              validation_data=(X_valid, y_valid),\n",
    "              callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 852us/step - loss: 0.3557\n"
     ]
    }
   ],
   "source": [
    "mse_test = keras_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo ora applicare un RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "121/121 [==============================] - 0s 843us/step - loss: 0.3597\n",
      "[CV] END learning_rate=0.004844485688815631, n_hidden=3, n_neurons=66; total time=   4.0s\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.3584\n",
      "[CV] END learning_rate=0.004844485688815631, n_hidden=3, n_neurons=66; total time=   3.5s\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.3541\n",
      "[CV] END learning_rate=0.004844485688815631, n_hidden=3, n_neurons=66; total time=   3.1s\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.4091\n",
      "[CV] END learning_rate=0.013109740391288613, n_hidden=2, n_neurons=32; total time=   5.6s\n",
      "121/121 [==============================] - 0s 849us/step - loss: 0.3643\n",
      "[CV] END learning_rate=0.013109740391288613, n_hidden=2, n_neurons=32; total time=   2.0s\n",
      "121/121 [==============================] - 0s 844us/step - loss: 0.3428\n",
      "[CV] END learning_rate=0.013109740391288613, n_hidden=2, n_neurons=32; total time=   3.4s\n",
      "121/121 [==============================] - 0s 951us/step - loss: 0.3517\n",
      "[CV] END learning_rate=0.00952217690072429, n_hidden=2, n_neurons=27; total time=   5.9s\n",
      "121/121 [==============================] - 0s 850us/step - loss: 0.3904\n",
      "[CV] END learning_rate=0.00952217690072429, n_hidden=2, n_neurons=27; total time=   2.1s\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.3489\n",
      "[CV] END learning_rate=0.00952217690072429, n_hidden=2, n_neurons=27; total time=   4.6s\n",
      "121/121 [==============================] - 0s 824us/step - loss: 0.4056\n",
      "[CV] END learning_rate=0.0035284734198815573, n_hidden=3, n_neurons=2; total time=  10.9s\n",
      "121/121 [==============================] - 0s 836us/step - loss: 0.6721\n",
      "[CV] END learning_rate=0.0035284734198815573, n_hidden=3, n_neurons=2; total time=   3.7s\n",
      "121/121 [==============================] - 0s 874us/step - loss: 1.3472\n",
      "[CV] END learning_rate=0.0035284734198815573, n_hidden=3, n_neurons=2; total time=   1.9s\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.3920\n",
      "[CV] END learning_rate=0.001369074180788194, n_hidden=2, n_neurons=67; total time=   6.9s\n",
      "121/121 [==============================] - 0s 985us/step - loss: 0.5252\n",
      "[CV] END learning_rate=0.001369074180788194, n_hidden=2, n_neurons=67; total time=   2.8s\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.3897\n",
      "[CV] END learning_rate=0.001369074180788194, n_hidden=2, n_neurons=67; total time=   4.8s\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cannot clone object <tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x000001EC1B9C1BB0>, as the constructor either does not set or modifies parameter learning_rate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-79df4feb575f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mrnd_search_cv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras_reg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_distribs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m rnd_search_cv.fit(X_train, y_train, epochs=50,\n\u001b[0m\u001b[0;32m      9\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                   \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\CSML21\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\CSML21\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    874\u001b[0m             \u001b[1;31m# we clone again after setting params in case some\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    875\u001b[0m             \u001b[1;31m# of the params are estimators as well.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 876\u001b[1;33m             self.best_estimator_ = clone(clone(base_estimator).set_params(\n\u001b[0m\u001b[0;32m    877\u001b[0m                 **self.best_params_))\n\u001b[0;32m    878\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\CSML21\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\CSML21\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mclone\u001b[1;34m(estimator, safe)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mparam2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparams_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mparam1\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mparam2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m             raise RuntimeError('Cannot clone object %s, as the constructor '\n\u001b[0m\u001b[0;32m     86\u001b[0m                                \u001b[1;34m'either does not set or modifies parameter %s'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                                (estimator, name))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Cannot clone object <tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x000001EC1B9C1BB0>, as the constructor either does not set or modifies parameter learning_rate"
     ]
    }
   ],
   "source": [
    "param_distribs = {\n",
    "    \"n_hidden\": [0, 1, 2, 3],\n",
    "    \"n_neurons\": np.arange(1, 100),\n",
    "    \"learning_rate\": reciprocal(3e-4, 3e-2),\n",
    "}\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=5, cv=3, verbose=2)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=50,\n",
    "                  validation_data=(X_valid, y_valid),\n",
    "                  callbacks=[keras.callbacks.EarlyStopping(patience=4)],\n",
    "                  verbose = 0\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!conda update tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
