{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import logit, expit\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From getting to pre-processing data\n",
    "In questa notebook affronteremo alcuni elementi che fanno parte di un progetto orientato al Machine Learning:\n",
    "1. Ottenere i dati (facilitato)\n",
    "2. Visualizzare i dati ottenuti\n",
    "3. Trasformare/ manipolare i dati per fornire un input agli algoritmi di ML\n",
    "\n",
    "A fine didattici/progetti personali, le fonti per ottenere dataset open sono molteplici:\n",
    "- Kaggle (https://www.kaggle.com/)\n",
    "- Machine Learning repository @ UC Irvine (https://archive.ics.uci.edu/ml/index.php)\n",
    "- Reddit: subreddit dataset (https://www.reddit.com/r/datasets/)\n",
    "\n",
    "Per questa prima esperienza utilizziamo un dataset ottenuto da Kaggle che affronta il problema dall **churn prediction**. Possiamo sintetizzare il problema in questo modo: *dato un insieme di clienti che hanno sottoscritto un servizio e caratterizzati/descritti da un insieme di proprieta', identificare un sottoinsieme di client che molto probabilmente abbondonera' in un dato momento nel futuro (di solito a breve termine) il servizio*. Il problema e' piuttosto generico in quanto riguarda in sostanza la risoluzione di un contratto.\n",
    "\n",
    "<p style=\"border: 3px solid crimson;box-shadow: 5px 6px indianRed;padding:20px;\">Formalmente ogni cliente $c$ e' descritto da un vettore $\\mathbf{c}=<c_1,\\ldots,c_n>$ t.c. $c_i\\in S_i$, dove $S_i$ e' un insieme, denota la proprieta' $i$-esima che caratterizza l'utente $c$. Per predirre qualora un generico utente con determinate proprieta' al tempo $t$, al tempo $t+\\Delta t$ abbandoni il servizio, possiamo **imparare** (learning) in modo automatico (machine) una funzione $f:S_1\\times\\ldots\\times S_n\\rightarrow \\{0,1\\}$, dove $0$ indica la permanenza nel servizio, viceversa $1$ indica l'abbandono del servizio.</p>\n",
    "\n",
    "Nel nostro caso affrontiamo il problema di churn prediction in ambito bancario: credit card churn prediction. \n",
    "\n",
    "Il dataset e' disponibile all'indirizzo https://www.kaggle.com/sakshigoyal7/credit-card-customers e riporta la seguente descrizione:\n",
    "```\n",
    "A manager at the bank is disturbed with more and more customers leaving their credit card services. They would really appreciate if one could predict for them who is gonna get churned so they can proactively go to the customer to provide them better services and turn customers' decisions in the opposite direction.\n",
    "This dataset consists of 10,000 customers mentioning their age, salary, marital_status, credit card limit, credit card category, etc. There are nearly 18 features.\n",
    "We have only 16.07% of customers who have churned. Thus, it's a bit difficult to train our model to predict churning customers.\n",
    "```\n",
    "\n",
    "### Definire l'obiettivo\n",
    "Lo sviluppo di un progetto che utilizzi ML solitamente e' orientato ad un **obiettivo** che non riguarda propriamente il ML, ma che utilizza le metodologie ML per **raggiungere** un'obiettivo. L'obiettivo determina:\n",
    "- come modello il problema\n",
    "    - supervisionato/non supervisionato, classificazione/regressione, batch/online, univariata/multivariata\n",
    "- quali modelli/algoritmi adottare\n",
    "    - fortemente dipendente dal punto precedente\n",
    "- quali misure di performance utilizzare nella valutazione del modello\n",
    "\n",
    "Nel nostro caso (credi card churn prediction) l'obiettivo e'molto specifico e ben definito, e la precedente trattazione ci porta a modellare il problema come un problema _supervisionato_ di _classificazione_ binaria in modalita' batch dato che il dataset e' facilmente caricabile in memoria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting and loading the data\n",
    "\n",
    "Per un'analisi preliminare del dataset e caricarlo in memoria utilizziamo il modulo **pandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = pd.read_csv('BankChurners.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Commento_: Potevo avere un'anteprima del file utilizzando anche un foglio di calcolo, dato che il formato e' CSV. E' utile verificare se la prima riga del file sia l'intestazione o header, e quindi contenere il nome delle colonne. Per verificare la presenza dello header posso utilizzare il comando head della shell\n",
    "\n",
    "Per avere una prima intuizione sul tipo di dati messi a disposizione, possiamo visualizzare le prime 5 righe del dataset utilizzando il metodo **head**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci sono 23 colonne.\n",
    "\n",
    "Per avere una piu'completa descrizione della struttura del dataset utilizziamo i metodi **info** oppure **describe**. IL primo metodo e' piu'orientato alla rappresentazione del dato, mentre il secondo metodo riporta le statistiche riguardanti le varie colonne, qualora siano di tipo numerico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confermiamo che abbiamo dati riguardanti 10127 clienti e in questo caso non abbiamo dati mancanti (Non-null count). I tipi di dati sono sia numerici sia categorici (Dtype)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per avere anche una prima impressione sulle colonne di tipo categorico/testuale possiamo utilizzare il metodo **value_counts**.\n",
    "\n",
    "ðŸ¤¡ Su diverse microblogging platforms trovare numerosi thread sulla controintuivita' del nome di questo metodo. SPOILER: la probabilita' di scriverlo correttamente al primo colpo e' prossima allo zero ðŸ¤¡."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset['Gender'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infine otteniamo il nome delle varie colonne in modo inferire - qualora non sia specificato in qualche documento - il significato della colonna. Tramite l'attributo **columns** otteniamo questa informazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Nome colonna | Significato|\n",
    "| --- | --- |\n",
    "|CLIENTNUM| ID del cliente|\n",
    "|Attrition_Flag| Indica se il cliente ha abbandonato (valore 1)|\n",
    "|Customer_Age| Eta' del cliente in anni|\n",
    "|Gender|Genere del cliente|\n",
    "|Dependent_count| Numero di dipendenti|\n",
    "|Education_Level| Livello di istruzione del cliente|\n",
    "|Marital_Status| Stato civile|\n",
    "|Income_Category| Livello di reddito in dollari|\n",
    "|Card_Category| Tipo di carta di credito|\n",
    "|Months_on_book| Numero di mesi di iscrizione al servizio|\n",
    "|Total_Relationship_Count| Numero di prodotti in possesso del cliente|\n",
    "|Months_Inactive_12_mon| Numero di mesi di inattivita' negli ultimi 12 mesi|\n",
    "|Contacts_Count_12_mon| Numero di contatti negli ultimi 12 mesi|\n",
    "|Credit_Limit| Limite di credito sulla carta|\n",
    "|Total_Revolving_Bal| Bilancio revolving|\n",
    "|Avg_Open_To_Buy|\n",
    "|Total_Amt_Chng_Q4_Q1| Differenza del totale delle transazioni tra Q4 e Q1|\n",
    "|Total_Trans_Amt| Totale delle transazioni su ultimi 12 mesi|\n",
    "|Total_Trans_Ct| Numero di transazioni negli ultimi 12 mesi|\n",
    "|Total_Ct_Chng_Q4_Q1| Differenza del numero di transazioni in Q4 rispetto a Q1|\n",
    "|Avg_Utilization_Ratio| ?|\n",
    "|Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1| Da eliminare|\n",
    "|Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2| Da eliminare|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un'analisi preliminare delle caratteristiche del dataset puo' avvenire in modo qualitativo attraverso la visualizzazione della distribuzione delle proprieta'. \n",
    "\n",
    "Utilizziamo il modulo **matplotlib** per la visualizzazione nel corso di questo laboratorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pandas il metodo **hist** permette una rapida visualizzazione della distribuzione delle proprieta' numeriche attraverso istogrammi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prima di visualizzare le distribuzioni, rimuoviamo le colonne che iniziano con Naive Bayes e la colonna con il codice cliente\n",
    "raw_dataset.drop(['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1','Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2','CLIENTNUM'],\n",
    "                 axis=1,\n",
    "                 inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset.hist(figsize=(22,9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dal momento che il dataset non e' troppo grande, possiamo valutare il coefficiente di correlazione di Pearson mediante il metodo **corr**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrice_correlazione = raw_dataset.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrice_correlazione['Customer_Age'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modificando il parametro _method_ posso cambiare il tipo di correlazione, verificando l'esistenza di una correlazione monotonica, non necessariamente lineare tra le proprieta'. Nel caso che vediamo utilizziamo il coefficiente di rank correlation di Spearman (Kendall rank correlation e' altra opzione)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrice_ranking_correlazione = raw_dataset.corr(method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrice_ranking_correlazione['Customer_Age'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ma il risultato non cambia :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the \"correlation\"\n",
    "Un'analisi qualitativa della correlazione e' effettuabile attraverso la visualizzazione delle distribuzioni congiunte - a coppie - delle propriet' di tipo numerico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scegliamo un sottoinsieme di caratteristiche che hanno un diverso livello di correlazione con l'eta' del cliente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colonne_interesse = ['Customer_Age', 'Months_on_book', 'Credit_Limit', 'Total_Trans_Amt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_matrix(raw_dataset[colonne_interesse], figsize=(22,9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per aumentare la leggibilita' e' possibile specificare la coppia di proprieta' di cui analizzare la distribuzione congiunta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset.plot(kind='scatter', x = 'Customer_Age', y='Total_Trans_Ct', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset.plot(kind='scatter', x = 'Customer_Age', y='Months_on_book', alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questa fase di sperimentazione posso definire delle proprieta' che **combinano** le proprieta' iniziali.\n",
    "\n",
    "Per esempio definiamo l'ammontare medio di una transazione engli ultimi 12 mesi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset['Avg_transaction'] = raw_dataset['Total_Trans_Amt']/raw_dataset['Total_Trans_Ct']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..e ne visualizziamo la distribuzione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset['Avg_transaction'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data for Machine Learning algorithms\n",
    "In questa sezione prepariamo il nostro dataset per essere utilizzato da un modello di ML - nel nostro caso un classificatore binario.\n",
    "\n",
    "**NB**: Prima di procedere, dobbiamo caricare un versione modificata del dataset orginale in cui sono state cancellate delle entry relative ad alcuni clienti e alcune collane. Tale modifica e' stata resa necessaria per introdurre alcuni step di data cleaning/data preparation relativi, che altrimenti non si sarebbero potuti applicare.\n",
    "\n",
    "Carichiamo la nuova versione del dataset ed analizziamo quali feature sono state corrotte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data = pd.read_csv('BankChurnersMissingData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le colonne 22 e 23 contengono molte entry con dati mancanti, mentre abbiamo dati mancanti sporadici nelle colonne Education Level, Months_Inactive_12_mon, Total_Revolving_Bal.\n",
    "\n",
    "Molti algoritmi di ML non gestiscono i **dati mancanti**, di conseguenza una fase della preparazione dei dati e' la definizione di funzioni che gestiscono i dati mancanti. Le opzioni piu' utilizzate sono 3:\n",
    "- rimuovo il sample che contiene le entry corrotte/mancanti => cancellazione della riga\n",
    "- rimuovo l'intera feature/colonna contenente i dati mancanti/corrotti => rimozione della colonna\n",
    "- imputo/inferisco i dati mancanti/corrotti mediante diverse strategie ben consolidate oppure originali\n",
    "\n",
    "Prima di gestire i dati mancanti, vediamo come **identificare** alcuni sample che contengono un dato mancante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_missing_credit = credit_data[credit_data['Customer_Age'].isnull()]\n",
    "sample_missing_credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminating samples with missing values\n",
    "Andiamo ad eliminare la riga relativa al cliente con un valore mancante nella colonna 'Total Revolving_Bal'. In questo caso non eseguiamo nessuna imputazione dal momento che togliamo un solo sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data.dropna(subset=['Total_Revolving_Bal'],\n",
    "                   inplace=True\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ho eliminato solo una riga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminating features with missing values\n",
    "Il dataset contiene due colonne che:\n",
    "1) contengono molti dati mancanti\n",
    "2) sono inutili al fine della classificazione in quanto contengono la predizione di un classificatore\n",
    "\n",
    "In questo caso possiamo tranquillamente eliminare le colonne. In generale, tutttavia, l'elminazione di una feature deve seguire da una scelta ponderata sulle rimanenti opzioni per la gestione dei dati mancanti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data.drop(['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1',\n",
    "                    'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2'],\n",
    "                   inplace=True,\n",
    "                   axis=1\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing missing values\n",
    "Abbiamo ancora una colonna che contiene 6 dati mancanti. In questo caso possiamo inferire il valore dei dati mancanti."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing missing values in Pandas\n",
    "Gestiamo l'imputazione del dato mancante in Pandas, mediante alcune statistiche e il metodo **fillna**.Ci concentriamo sulla colonna 'Customer_Age'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_age = credit_data['Customer_Age'].median()\n",
    "median_age\n",
    "# credit_data['CustomerAge'].fillna(median_age, inplace=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing missing values in Scikit-Learn\n",
    "Scikit-learn - SKL - offre una soluzione piu' completa per gestire l'imputazione dei dati mancanti.\n",
    "\n",
    "La classe **SimpleImputer** fornisce una strategia base. I valori mancanti possono essere imputati da valori costanti oppure statistiche puntuali.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='median')\n",
    "imputer.fit(credit_data['Customer_Age'].values.reshape(-1, 1))\n",
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il metodo fit non esegue alcuna sostituzione ma esegue solamente la strategia utilizzata sul vettore o matrice che ricve in input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data['Customer_Age'] = imputer.transform(credit_data['Customer_Age'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il metodo transform applica la strategia al vettore/matrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abbiamo un ulteriore problema costituito dalla colonna 'Income_Category'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn API e' organizzato secondo una serie di principi che ne ispirano il design e l'architettura:\n",
    "\n",
    "1) **Consistency**: ogni oggetto condivide un'interfaccia consistente e condivisa\n",
    "    - **Estimator**: un qualsiasi oggetto che stima alcuni parametri. La stima e' eseguita dal metodo **fit** che richiede un dataset come parametro\n",
    "    - **Transformers**: un estimator che trasforma un dataset. La trasformazione e' eseguita dal metodo **transform** che richiede il dataset da trasformare come parametro. Restuisce il dataset trasformato. Esiste un metodo **fit_trasform** che esegue entrambi i metodi ed e'ottimizzato.\n",
    "    - **Predictors**: un oggetto estimator che predice. Un predictor ha il metodo **predict** che richiede un insieme di sample nuovi e restituisce le predizioni associate.\n",
    "2) **Inspection**: tutti gli iperparametri sono accessibili mediante attributi cosi' come i parametri appresi. Si utilizza la notazione con underscore\n",
    "3) **Nonproliferation of classes**: datasets sono Numpy arrays o matrici sparse Scipy. Non ci sono oggetti dataset definiti da utenti\n",
    "4) **Composition**: i moduli/classi rilasciati sono componibili\n",
    "5) **Sensible defaults**: si devono definire dei valori di default ragionevoli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data['Income_Category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una possibile soluzione e' quella di inferire i valori mancanti sulla base delle feature. In questo caso dobbiamo risolvere un problema di classificazione/regressione utilizzando le entry con i valori non mancanti come label. SKL mettere a disposizione la classe sperimentale **IterativeImputer** (https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html#sklearn.impute.IterativeImputer), tuttavia non e' possibile utilizzarlo con in valori categorici che abbiamo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_it = IterativeImputer(max_iter=10, random_state=0)\n",
    "imputer_it.fit(credit_data[['Gender','Income_Category']].values)\n",
    "IterativeImputer(random_state=0)\n",
    "y = credit_data[['Gender','Income_Category']].values\n",
    "imputer_it.transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questo caso applichiamo una soluzione piu' 'naive' in cui i valori unknown vengono sostituiti in modo casuale, rispettando la distribuzione delle rimanenti livelli di reddito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_ic = credit_data[credit_data['Income_Category'] != 'Unknown']['Income_Category'].value_counts()\n",
    "v, f = count_ic.index, count_ic.values/count_ic.sum()\n",
    "credit_data.loc[credit_data['Income_Category'] == 'Unknown','Income_Category'] = np.random.choice(v,size=1112,p=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data['Income_Category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling categorical data\n",
    "Per ora abbiamo trattato principalmente dati numerici. Tuttavia, dal momento che molti algoritmi di apprendimento non trattano dati categori o testuali, dobbiamo codificare i dati categorici in dati numerici.\n",
    "\n",
    "Un primo approccio per convertire un dato categorico in un dato categorico in un dato numerico e'utilizzare la classe **OrdinalEncoder**, la quale assegna ad ogni categoria un valore numerico distinto.\n",
    "\n",
    "Applichiamo la classe alla colonna 'Card_Category'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_enc = OrdinalEncoder()\n",
    "credit_data['Card_Category'] = ord_enc.fit_transform(credit_data['Card_Category'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le categorie identificate sono associate all'attributo **categories_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_enc.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un problema di questo tipo di trasformazione e' l'introduzione di un ordinamento non presente - caso precedente, o il cambiamento dell'ordinamento. \n",
    "\n",
    "Trattiamo il secondo caso, trasformando la colonna categorica'Income_Category'. Nonostante sia una feature categorica, le fasce di reddito sono crescenti quindi esiste un ordinamento. Per rispettare questo ordinamento utilizziamo sempre la classe OrdinalEncoder, ma definiamo a priori la lista ordinata delle categorie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_cat = [['Less than $40K','$40K - $60K', '$60K - $80K', '$80K - $120K', '$120K +']]\n",
    "ord_enc_income = OrdinalEncoder(categories=rank_cat)\n",
    "credit_data['Income_Category_Num'] = ord_enc_income.fit_transform(credit_data['Income_Category'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data[['Income_Category_Num','Income_Category']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per evitare l'introduzione di un ordimento fittizio, si crea una feature binaria per ogni categoria. Per ogni sample, viene posta a 1 la feature/colonna che corrisponde alla categoria originaria. Questo tipo di codifica viene detta **one-hot encoding** ed implementata dalla classe **OneHotEncoder** (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edu_enc = OneHotEncoder()\n",
    "edu_ohe = edu_enc.fit_transform(credit_data['Education_Level'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edu_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edu_ohe.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
